#!/usr/bin/python
# -*- coding: utf-8 -*-
from gensim import corpora
from collections import defaultdict
from gensim import models
from gensim import similarities
from  matplotlib import pyplot as  plt
import jieba

def Show2dCorpora(corpus):

    nodes = list(corpus)
    print(len(nodes))
    ax0 = [x[1][1] for x in nodes] # 绘制各个doc代表的点
    ax1 = [x[2][1] for x in nodes]
    # print(ax0)
    # print(ax1)
    plt.plot(ax0,ax1,'o')
    plt.show()

#构建语料库
raw_corpus = ["Human machine interface for lab abc computer applications",
              "A survey of user opinion of computer system response time",
              "The EPS user interface management system",
              "System and human system engineering testing of EPS",
              "Relation of user perceived response time to error measurement",
              "The generation of random binary unordered trees",
              "The intersection graph of paths in trees",
              "Graph minors IV Widths of trees and well quasi ordering",
              "Graph minors A survey"]

raw_documents = [
    '0南京江心洲污泥偷排”等污泥偷排或处置不当而造成的污染问题，不断被媒体曝光',
    '1面对美国金融危机冲击与国内经济增速下滑形势，中国政府在2008年11月初快速推出“4万亿”投资十项措施',
    '2全国大面积出现的雾霾，使解决我国环境质量恶化问题的紧迫性得到全社会的广泛关注',
    '3大约是1962年的夏天吧，潘文突然出现在我们居住的安宁巷中，她旁边走着40号王孃孃家的大儿子，一看就知道，他们是一对恋人。那时候，潘文梳着一条长长的独辫',
    '4坐落在美国科罗拉多州的小镇蒙特苏马有一座4200平方英尺(约合390平方米)的房子，该建筑外表上与普通民居毫无区别，但其内在构造却别有洞天',
    '5据英国《每日邮报》报道，美国威斯康辛州的非营利组织“占领麦迪逊建筑公司”(OMBuild)在华盛顿和俄勒冈州打造了99平方英尺(约9平方米)的迷你房屋',
    '6长沙市公安局官方微博@长沙警事发布消息称，3月14日上午10时15分许，长沙市开福区伍家岭沙湖桥菜市场内，两名摊贩因纠纷引发互殴，其中一人被对方砍死',
    '7乌克兰克里米亚就留在乌克兰还是加入俄罗斯举行全民公投，全部选票的统计结果表明，96.6%的选民赞成克里米亚加入俄罗斯，但未获得乌克兰和国际社会的普遍承认',
    '8京津冀的大气污染，造成了巨大的综合负面效应，显性的是空气污染、水质变差、交通拥堵、食品不安全等，隐性的是各种恶性疾病的患者增加，生存环境越来越差',
    '9 1954年2月19日，苏联最高苏维埃主席团，在“兄弟的乌克兰与俄罗斯结盟300周年之际”通过决议，将俄罗斯联邦的克里米亚州，划归乌克兰加盟共和国',
    '10北京市昌平区一航空训练基地，演练人员身穿训练服，从机舱逃生门滑降到地面',
    '11腾讯入股京东的公告如期而至，与三周前的传闻吻合。毫无疑问，仅仅是传闻阶段的“联姻”，已经改变了京东赴美上市的舆论氛围',
    '12国防部网站消息，3月8日凌晨，马来西亚航空公司MH370航班起飞后与地面失去联系，西安卫星测控中心在第一时间启动应急机制，配合地面搜救人员开展对失联航班的搜索救援行动',
    '13新华社昆明3月2日电，记者从昆明市政府新闻办获悉，昆明“3·01”事件事发现场证据表明，这是一起由新疆分裂势力一手策划组织的严重暴力恐怖事件',
    '14在即将召开的全国“两会”上，中国政府将提出2014年GDP增长7.5%左右、CPI通胀率控制在3.5%的目标',
    '15中共中央总书记、国家主席、中央军委主席习近平看望出席全国政协十二届二次会议的委员并参加分组讨论时强调，团结稳定是福，分裂动乱是祸。全国各族人民都要珍惜民族大团结的政治局面，都要坚决反对一切危害各民族大团结的言行',
    '16近日,中共中央文献研究室编辑了《习近平关于社会主义社会建设论述摘编》。本书内容摘自习近平同志二0一二年十一月十五日至二0一七年九月十九日期间的讲话',
    '17习近平在学习贯彻党的十九大精神研讨班开班式上发表重要讲话',
    '中国共产党第十九次全国代表大会（简称党的十九大）于2017年10月18日至10月24日在北京召开。2017年10月18日上午9:00，中国共产党第十九次全国代表大会在人民大会堂开幕。',
    '十九大报告提出,围绕新时代坚持和发展什么样的中国特色社会主义'
]

#过滤停用词
def stopwordslist(filepath):
    stopwords = [line.lower().strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]
    return stopwords

stopwords = stopwordslist('D:\\机器学习\\深度学习\\my_code\\stop_word_list')  # 这里加载停用词的路径

#jieba切词并过滤停用词
def seg_sentence(sentence):
    sentence_seged = jieba.cut(sentence.strip())
    outstr = []
    for word in sentence_seged:
        if word not in stopwords and  not word.isdigit() and '%' not in word:
            if word != '\t':
                outstr.append(word)
    #print(','.join(sentence_seged))
    print(outstr)
    return outstr

#对语料库进行切词、去除停用词，使用map spark 便于分布式化
corpus=list(map(lambda sentence:seg_sentence(sentence),raw_documents))
precessed_corpus=corpus
print(corpus)

'''
#过滤只出现一次的词，生成词典
frequency = defaultdict(int)
for text in corpus:
    for token in text:
        frequency[token] += 1

#precessed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]
precessed_corpus = [[token for token in text ] for text in corpus]

precessed_corpus
'''
# 将语料库生成词典索引
dictionary = corpora.Dictionary(precessed_corpus)
print(dictionary)
print (dictionary.token2id)

#构建基于语料库的词袋，之所以叫词袋，是因为袋子里面的词之间顺序，并没有被表示
bow_corpus = [dictionary.doc2bow(text) for text in corpus ]
print('-'*100)
print(bow_corpus)
#创建tf-idf模型，以语料库的词袋为参数
tfidf_model = models.TfidfModel(bow_corpus,id2word=dictionary,dictionary=dictionary)
print(list(tfidf_model[bow_corpus]))


#显示在语料库内，每个句子内的词的tf-idf值
print('start iter corpus !\n')
list1=[]
for x in bow_corpus:
    g=tfidf_model[x]
    ll=list(map(lambda x :(dictionary.get(x[0]),x[1]),g))
    list1.append(ll)

print(list1)

#另外一种实现方法，tfidf[x],x参数可以是词袋本身
print('another way ')
list2=[]
for x in tfidf_model[bow_corpus]:
    ll=list(map(lambda y :(dictionary.get(y[0]),y[1]),x))
    list2.append(ll)

print(list2)

# 其他句子的tf-idf值：
string = "system minors"
string_bow = dictionary.doc2bow(string.lower().split())
string_tfidf = tfidf_model[string_bow]
print([string,string_bow])
print ([string,string_tfidf])
#文档相似
#index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=12)


# lsi or lsa model
print('-'*40)
print('start lsi \n')
corpus_tfidf=tfidf_model[bow_corpus]
lsi_model=models.LsiModel(corpus_tfidf,id2word=dictionary,num_topics=100)
corpus_lsi=lsi_model[corpus_tfidf]
lsi_model.save('/tmp/model.lsi')
lsi_model = models.LsiModel.load('/tmp/model.lsi')
print(corpus_lsi)
print(lsi_model.get_topics())
print(lsi_model.print_topics())
print(lsi_model.show_topics())

#Show2dCorpora(corpus_lsi)

## LDA模型 **************************************************
lda_model = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=100)
corpus_lda = lda_model[corpus_tfidf]

#print(list(corpus_lda))
for doc in corpus_lda:
    print(doc)
#Show2dCorpora(corpus_lda)
# nodes = list(corpus_lda)
# pprint(list(corpus_lda))

print('-'*100)
print('start similarities  \n')

查找目标
def similarity(list_tfidf,list_lsi,list_lda):
    similarity_tfidf=similarities.MatrixSimilarity(corpus_tfidf, num_best=5)
    print(similarity_tfidf[list_tfidf])

    similarity_lsi=similarities.MatrixSimilarity(corpus_lsi,num_best=5)
    print(similarity_lsi[list_lsi])

    similarity_lda=similarities.MatrixSimilarity(corpus_lda,num_best=5)
    print(similarity_lda[list_lda])

similarity(corpus_tfidf,corpus_lsi,corpus_lda)


str='习近平总书记驱车2个多小时,从西昌市来到位于大凉山深处的昭觉县三岔河乡三河村'
str='《习近平总书记系列重要讲话读本》之:打铁还需自身硬 党的十八大以来,我们党坚持党要管党'
#str='习近平'
#对str进行切词，去除停用词
sentence=list(map(lambda sentence:seg_sentence(sentence),[str]))
#基于原始的语料库所得到的词袋，生成str句子内每个单词的词频
bow_str = [dictionary.doc2bow(text) for text in sentence]
print(bow_str)
#得到str的tfidf值
str_tfidf=tfidf_model[bow_str]
#将tfidf值传入各个模型，获得str的相似句子list
similarity(str_tfidf,lsi_model[str_tfidf],lda_model[str_tfidf])
